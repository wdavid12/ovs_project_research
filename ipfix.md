# Introduction
This document will describe the IPFIX implementation in OVS and the possibility
of extending it for the purposes of sampling packets on-demand. Currently, OVS
has a sampling mechanism that only extracts metadata from packets, and does not
allow to send the packet contents to the IPFIX collector.

# High Level Overview
An OVS switch can be divided into 2 major components:
- The datapath
- An OpenFlow bridge

The datapath is the component that is in charge of directly processing packets.
In our case, the datapath in implemented as a kernel module, but other
possibilities exist (DPDK for instance).

The datapath maintains a cache of OpenFlow flow entries in the form of hash
tables. When a packet is recieved, the hash tables are searched for a matching
entry and the associated actions are executed. If a match cannot be found, the
datapath will call the OpenFlow bridge in a process called an _upcall_.

The upcall will be handled by userspace, and the actual OpenFlow tables will be
searched in order to find the appropriate action. In this stage a miss can
occur again, and the SDN controller will have to handle the packet. This detail
is not important for the purpose of this document.

OVS has extended the OpenFlow protocol with various actions, including the
`sample` action. This action allows us to sample packets from a specific flow
and pass them to the IPFIX implementation in userspace.

Internally, the `sample` action will cause the datapath to generate an upcall
whenever a packet needs to be sampled. The userspace component handling the
upcall will be in charge of extracting the necessary information from the
packet and passing it along to the IPFIX implementation.

The datapath (kernel module) and OpenFlow bridge implementations can be found
in the `datapath` and `ofproto` directories respectively.

We will begin our investigation by looking at the function responsible for
handling upcalls.

# upcall Handling
## process_upcall()
upcalls in OVS are handled by `process_upcall` in `ofproto/ofproto-dpif-upcall.c`.
We can see that a `FLOW_SAMPLE_UPCALL` (the type of upcall that will be
generated by the `sample` action) is handled by calling
`dpif_ipfix_flow_sample`. This function recieves (among other parameters) the
packet itself, which means that we may be able to hook it in order to send the
packet contents to the IPFIX collector. The function also recieves a `struct dpif_ipfix*`
that is extracted from the corresponding bridge elsewhere.

## dpif_ipfix_flow_sample()
`dpif_ipfix_flow_sample` can be found in `ofproto/ofproto-dpif-ipfix.c` line 2752.
We can see that the code approximates the number of matched packets using
the probablity and finds the IPFIX exporter that corresponds to the
`collector_set_id` which we have configured in our `sample` action. After this
intial processing, `dpif_ipfix_sample` will be called.

## dpif_ipfix_sample()
`dpif_ipfix_sample` can be found in `ofproto/ofproto-dpif-ipfix.c` line 2658.
This function is responsible for allocating a cache entry for the flow and then
calling `ipfix_cache_update`. Note that the packet is still available at this
stage, but subsequent functions in the call chain (including
`ipfix_cache_update`) will not recieve the packet.

It is possible to add a linked list of packets to
`struct ipfix_flow_cache_entry` so that we can keep the packets and send them
later.  We must save a __copy__ of the packet, since the packet will be freed
later by code in `ofproto-dpif-upcall` (after the entire call chain completes).

## small detour: cache entries
It seems that the IPFIX implementation in OVS maintains a cache entry per
sampled flow. This is done in order to maintain the various counters that will
be passed to the IPFIX collector. The entries are held in a hash table and a
linked list ordered by timestamp.

## ipfix_cache_update()
`ipfix_cache_update` can be found in `ofproto/ofproto-dpif-ipfix.c` line 1983.
Here we update the metadata for the flow using `ipfix_cache_aggregate_entries`
and call `ipfix_update_stats`. Note that the new entry is freed at this stage
(unless it is a new entry) and it will not be available from now on.
`ipfix_update_stats` simply updates a few counters (total packets recieved).

# How is the data sent?
Since the sampling control flow only updates metadata in internal data
structures, we must now ask ourseleves who is responsible for generating IPFIX
traffic. Looking at the API of `ofproto-dpif-ipfix.c`, we see that
`dpif_ipfix_run` is expected to be called at regular intervals. This function
is called by the `run` function in `ofproto-dpif.c`, which seems to be the main
entry point of the OpenFlow bridge implementation. This function is probably
run continously as part of the OVS event loop. Further investigation is
required since the `run` function is not called direcly, rather a function table
is used in an object-oriented style.

We shall focus on the `dpif_ipfix_run` function and assume it is invoked at
regular intervals.

## dpif_ipfix_run
`dpif_ipfix_run` can be found in `ofproto/ofproto-dpif-ipfix.c` line 2880.
This function will call `dpif_ipfix_cache_expire` for each flow cache entry.

## dpif_ipfix_cache_expire
`dpif_ipfix_cache_expire` can be found in `ofproto/ofproto-dpif-ipfix.c` line 2798.
Finally we get to the code responsible for sending IPFIX messages. We see that
for each flow cache entry, a template message is possibly sent (if it hasn't
been sent recently) and data message containing the counters will also be sent.
The templates data format are hardcoded and can be found in the same file, (for
instance, look at `ipfix_define_template_fields` in line 1393.

## Plan of action for extending the protocol

### Option 1: Hook IPFIX flow, don't reuse IPFIX code
As stated above, we can keep a linked list of packets in every cache flow
entry, (or even outside it in a global data structure). When
`dpif_ipfix_cache_expire` is called, we will also send each packet in our
linkes list to the same collector (different fixed port),
using a minimal udp wrapper:
	uint16_t flow_id;
	uint32_t sample_seqnum;
	uint8_t *data;
In the future maybe we can add parameters to the `sample()` action in order to
'turn off' the regular IPFIX packets and only send our own.
Maybe we could also add ovsdb fields to configure the IP/port of our collector.

### Option 2: Using IPFIX
Again, we must keep a linked list of packets (__copies__ of course) in the flow
cache entries. We will need to define a new IPFIX template to support sending
the truncated packet itself. We can add a new function,
`ipfix_send_packet_trunc` that will go over our linked list and send each of
them as an IPFIX record.

Note that there is a global mutex that protects all of
the data structures. This mutex is locked at the time of
`dpif_ipfix_cache_expire`, so we do not need to worry about race conditions.

Note also that will have to send our new template before we send any packet
data. We can add it to `ipfix_def_options_template_fields`

suggestion:
```
diff --git a/ofproto/ofproto-dpif-ipfix.c b/ofproto/ofproto-dpif-ipfix.c
index 4d9fe787f..e831cb342 100644
--- a/ofproto/ofproto-dpif-ipfix.c
+++ b/ofproto/ofproto-dpif-ipfix.c
@@ -188,6 +188,7 @@ BUILD_ASSERT_DECL(sizeof(struct ipfix_header) == 16);

 enum ipfix_options_template {
     IPFIX_OPTIONS_TEMPLATE_EXPORTER_STATS = 0,
+    IPFIX_OPTIONS_TEMPLATE_EXPORTER_TRUNC = 1,
     NUM_IPFIX_OPTIONS_TEMPLATE
 };

@@ -1379,6 +1380,11 @@ ipfix_def_options_template_fields(enum ipfix_options_template opt_tmpl_type,
         return ipfix_def_exporter_options_template_fields(opt_tmpl_hdr_offset,
                                                           msg);
         break;
+    case IPFIX_OPTIONS_TEMPLATE_EXPORTER_TRUNC:
+	/* TODO: implement this function */
+	return ipfix_def_exporter_options_trunc_template(opt_tmpl_hdr_offset,
+			msg);
+	break;
     case NUM_IPFIX_OPTIONS_TEMPLATE:
     default:
         OVS_NOT_REACHED();
@@ -2800,6 +2806,7 @@ dpif_ipfix_cache_expire(struct dpif_ipfix_exporter *exporter,
                         const uint32_t export_time_sec)
 {
     struct ipfix_flow_cache_entry *entry, *next_entry;
+    struct dp_packet *packet, *next_packet;
     uint64_t max_flow_start_timestamp_usec;
     bool template_msg_sent = false;
     enum ipfix_flow_end_reason flow_end_reason;
@@ -2849,6 +2856,16 @@ dpif_ipfix_cache_expire(struct dpif_ipfix_exporter *exporter,
         /* XXX: Group multiple data records for the same obs domain id
          * into the same message. */
         ipfix_send_data_msg(exporter, export_time_sec, entry, flow_end_reason);
+
+	/* Send sampled packets */
+	LIST_FOR_EACH_SAFE(packet, next_packet, sampled_packet_list_node,
+			&entry->sampled_packets_list) {
+		/* TODO: implement this function */
+		ipfix_send_sampled_packet_msg(exporter, exporter_time_sec,
+				packet);
+		free(packet);
+	}
+
         free(entry);
     }
 }
```
